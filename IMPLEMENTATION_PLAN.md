Metadata and Frameworks for Agentic Automation Projects
Schema.org and W3C Standards for Project Metadata
Schema.org (Project/ResearchProject): Schema.org provides a generic Project type (under Organization) to represent an “enterprise, planned to achieve a particular aim”[1]. A specialized ResearchProject subtype exists (in development) for scientific projects[2]. Using Schema.org’s project schema (with properties like description, startDate, endDate, funder, etc.) helps encode basic project definitions (goals, timeline, funding) in a web-standard way. This can align with the Agentic Canvas “Project/Task Definition” segment by capturing the project’s scope and objectives in a structured form. Additionally, schema.org’s Grant type can link funding sources[3], and CreativeWork or Dataset types can describe key outputs or resources. The Bioschemas initiative extends schema.org for life sciences, and could inform descriptions of biomedical studies or datasets within projects (e.g. a Bioschemas Study profile) for better interoperability[4]. Integrating Schema.org in the canvas ensures broad compatibility and discoverability of project metadata (many platforms index schema.org), and avoids reinventing basic project descriptors.
DCAT (Data Catalog Vocabulary): W3C’s DCAT is a standard RDF vocabulary to describe datasets and data services in catalogs[5]. While DCAT focuses on data, it’s relevant for agentic automation projects which often produce or consume datasets. DCAT (especially the European DCAT-AP profile) can be used to catalog datasets involved in the project, including metadata like data owner, format, and license. Crucially, DCAT leverages Dublin Core properties for access rights and licenses (e.g. dct:accessRights, dct:license) to indicate data sensitivity or sharing constraints. This supports the Data Access & Sensitivity aspect of the canvas – one can record if a dataset is open, restricted, or confidential, and any privacy/security constraints. Aligning to DCAT (as adopted in EOSC data portals[6]) makes project data assets easier to find and integrate in the broader open science ecosystem. The EOSC metadata profiles (like DCAT-AP and domain-specific extensions such as HealthDCAT-AP) are worth following to ensure compatibility with EU infrastructure. Using DCAT in the canvas context means each data input/output of an automation can be a DCAT Dataset with standard fields (and possibly DOIs via DataCite metadata), rather than a custom ad-hoc description.
PROV-O (Provenance Ontology): W3C’s PROV-O provides a rich model for process provenance, which is directly useful for representing tasks, activities, and outcomes. PROV’s core classes – Activity (an action or task over time), Entity (a data or result artifact), and Agent (a person or software agent responsible) – allow us to capture who did what, when, and using which inputs[7][8]. For an agentic automation project, PROV-O can model each automated task or experiment as an Activity linked to input datasets (prov:used) and output results (prov:generated). This aligns with Task Definitions and Outcomes in the canvas, ensuring that every outcome has traceable provenance (which dataset or step produced it, which agent was involved[8][9]). PROV-O also supports linking activities (e.g. wasInformedBy to show one task depends on another)[10], useful for modeling project workflows or governance stages. Furthermore, PROV includes a notion of Plan: a prov:Plan is “an entity that represents a set of actions or steps intended… to achieve some goals”[11]. In our context, a Plan could represent the user’s expected procedure or requirements that guide the automation. By associating a Plan with an Activity (prov:wasAssociatedWith/prov:hadPlan), we can record the intended behavior vs. the actual execution.
P-Plan (Plan Ontology): While PROV-O defines prov:Plan abstractly, the P-Plan extension provides a richer way to link plans to provenance. P-Plan was created “to represent the plans that guided the execution of scientific processes,” describing plan components and mapping them to the provenance of what actually happened[12][13]. It lets us detail a workflow template or design (with steps and expected inputs/outputs) and then tie each step to the corresponding executed Activity recorded in PROV. This is highly relevant to aligning user expectations vs. developer implementation: “The plan can describe the expectations for the execution, which can then be contrasted with the provenance to detect deviations”[13]. In the canvas, one could use P-Plan to formalize the “User Expectations” section as a set of expected tasks/outcomes, and later fill in PROV traces to evaluate how the real system’s behavior matched the plan. This approach supports iterative governance: discrepancies between plan and execution can flag issues in feasibility or requirement creep.
Alignment Tip: By using PROV-O/P-Plan, the governance staging of a project (design -> development -> evaluation) can be represented as a series of Activities with associated plans and responsible Agents. Governance checkpoints (e.g., review meetings, approvals) can be modelled either as special Activities (with a prov:Agent like a review board) or by annotating Activities with a stage property (e.g., a custom or existing term for project phase). This structured timeline makes it easier to apply standards like the PROV Access and Usage guidelines or even hook into FAIR Digital Object provenance. PROV’s widespread adoption in scientific workflows means our canvas metadata can readily integrate with provenance stores or workflow logs.
Research & Biomedical Project Ontologies
FRAPO (Funding, Research Administration & Projects Ontology): FRAPO is an ontology designed to describe the administrative metadata of research projects[14]. It is CERIF-compliant (aligning with the Common European Research Information Format used in many CRIS systems) and includes classes for projects and their grants, funding agencies, partners, project budget, deliverables, etc.[15][16]. This ontology can enrich the canvas’ Project Definition and Governance sections by capturing things like project status (active, completed), deliverables (tangible outputs promised – maps to “Outcomes”), and relationships to organizations or collaborators. For example, FRAPO has a frapo:Project class for the endeavor itself and associated properties for start/end dates, objectives, and a frapo:deliverable for expected outputs. It also works with SCoRO (Scholarly Contributions & Roles Ontology) to specify contributions and roles over time[17] – useful for documenting which team members (or AI agents) fulfilled certain roles, addressing the developer capabilities and responsibilities aspect. By integrating FRAPO, the Agentic Canvas can avoid redefining common project entities (funders, budgets, etc.) and remain compatible with how universities and funding bodies represent project metadata. Mapping: The canvas “governance staging” might correspond to FRAPO’s notion of project lifecycle stage or status (though FRAPO’s status class is generic). If needed, one could extend FRAPO with a controlled vocabulary for stage/gate names, or simply use FRAPO’s frapo:Project linked to PROV Activities for each stage.
Importantly, FRAPO explicitly does not cover scientific investigation details – “FRAPO is not intended to describe the details of research investigations… which are the domain of a complementary ontology, ISA-RDF”[18]. For the biomedical context, this points to using the ISA model or OBI ontology in tandem with FRAPO. The ISA (Investigation/Study/Assay) model structures experimental metadata: an Investigation (top-level project) contains Studies (sub-projects or use-cases) which have Assays (specific experiments/data-generating tasks). The OBO Ontology for Biomedical Investigations (OBI) provides a rich library of terms to describe study designs, protocols, instruments, data types, and outcomes in biomedicine[19][20]. By mapping the “Tasks” and “Outcomes” of an agentic system in a biomedical setting to OBI/ISA terms, we ensure the metadata can express, for instance, the assay type performed by the agent, the material processed, the measured outcome, etc., with unambiguous semantics. This aligns with the canvas sections on tasks and data: an Agentic automation might be performing an OBI:planned process (like a diagnostic assay), consuming some material entity (patient data) and yielding a measurement datum. If the project already uses ISA-Tab or similar, adopting ISA-RDF for the canvas will prevent duplication; the canvas can essentially point to an ISA metadata bundle for detailed experiment info, while focusing on higher-level governance and expectation management.
Domain Application Profiles: The EU’s EURIO (European Research Information Ontology) is another example – it combines multiple ontologies (including FRAPO) to describe both administrative and result information of EU-funded projects[21][22]. Adhering to such profiles or at least being compatible means the canvas metadata could be exported to (or imported from) systems like CORDIS or institutional CRIS. In practice, this means using standard properties for things like project ID, funding call, organization IDs, and linking to publications/datasets. The canvas Outcomes section, for instance, can leverage FaBiO (FRBR-aligned Bibliographic Ontology) for publications or datasets produced[23], instead of creating a new category for outputs. Overall, reusing research project ontologies ensures that when comparing agentic projects, we can easily align their basic info (size, funding, domain) and focus on the novel agent-specific metadata.
Data Access and Sensitivity Standards
Agentic automation projects, especially in biomedical domains, must carefully document data governance – what data is used, under what restrictions, and with what privacy safeguards. Beyond DCAT (for dataset cataloging), there are specialized schemas and ontologies to capture data sensitivity, consent, and usage policies:
•	Data Use Ontology (DUO): The GA4GH DUO is a standard ontology for tagging datasets with permitted use conditions. “DUO provides a standard set of terms that can be used to tag datasets with use permissions, aiding researchers in discovering data” and helping data access committees decide on access[24]. In our context, if an agentic system uses sensitive biomedical data, we can tag that dataset with DUO codes (e.g., health/medical research only, no commercial use, requires ethics approval). This adds machine-readable clarity to the Data Access/Sensitivity canvas field. Rather than a free-text note about “data is patient-level and requires consent”, we can specify a DUO term like DUO:0000021 (Health/Medical/Biomedical Research Use Only) as the dataset’s permission. This not only avoids ambiguity but aligns with international genomic data governance standards[25]. Many repositories (EGA, dbGaP) already use DUO, so an agentic project reusing those datasets should inherit the DUO tags to remain compliant.
•	Machine-Actionable DMPs: The Research Data Alliance’s output on machine-actionable Data Management Plans (maDMP) provides a structured way to record data handling and governance decisions[26]. A DMP typically covers what data will be collected, storage, who can access, and any ethical or security measures. The RDA DMP Common Standard (with an ontology, DCSO) includes core concepts like Dataset, DataSensitivity, SecurityMeasures, EthicalCompliance, Cost etc. For example, a DMP might specify “Data will be archived in repository X under license Y; personal data will be anonymized; access is restricted to approved collaborators”. By including these elements, the canvas can track data governance staging: which steps have been taken to secure data or obtain approvals. We can integrate relevant maDMP fields into Governance or Data sections of the canvas (or link to an existing DMP document). Because maDMPs are typically JSON-LD, one could even embed a JSON-LD snippet following the DMP Common Standard, ensuring the information is both human-readable in the canvas and machine-actionable. As the RDA notes, “traditional DMPs’ workload can be reduced when they become machine-actionable”[26] – meaning easier updates and consistency. Embracing this in our project metadata avoids redundant entry of data handling info and aligns with the FAIR principle of making metadata richly linked.
•	Security and Sensitivity Taxonomies: Many organizations classify data by sensitivity levels (public/internal/confidential/highly restricted, etc.). While no single ontology is universally adopted, if the agentic project uses such labels, we should map them to a standard or at least document the schema. For instance, the canvas could use a property like dct:accessRights with values from a controlled list (perhaps the four-level scheme of a university or the GDPR-oriented categories). If more granularity is needed (e.g., PHI vs PII vs public data), one might reference the Health Level 7 (HL7) data sensitivity codes or ISO standards for information classification. The key is to flag sensitive data in a structured way: e.g. an image dataset might be marked with “contains personal data = true” and linked to a consent form record. Standards like DUO help here (as do efforts like GDPR-specific ontologies), but at minimum the canvas should include fields for data sensitivity classification and required safeguards. This ensures that when comparing projects, one can easily filter those dealing with high-risk data and see how they managed it.
•	Croissant (ML Dataset Metadata): As a modern standard, Croissant is directly relevant if our agentic automation involves ML datasets. Croissant is “an open community-built standardized metadata vocabulary for ML datasets, including key attributes and properties of datasets”, built as a schema.org extension[27]. It covers not just descriptive info but also machine-useful details for loading data in ML frameworks. By using Croissant to describe datasets in the project (which can be embedded as JSON-LD in the canvas or referenced), we gain a few advantages: (a) Discoverability – Croissant-annotated datasets can be indexed by Google Dataset Search and others[28], increasing the visibility of the project’s data; (b) Interoperability – the agent can load data according to Croissant specs, meaning minimal friction moving between tools (important for the developer feasibility aspect, as it reduces data wrangling effort); (c) Extension for RAI (Responsible AI) – Croissant is planning extensions for responsible AI metadata[29][30], which could include documenting bias, provenance, etc. This aligns with canvas sections on governance and outcomes (ensuring we document any ethical audits or bias evaluations of the data). In summary, adopting Croissant where applicable avoids duplicating a dataset schema and taps into an emerging community standard backed by MLCommons and others. Since Croissant inherits schema.org, it will dovetail with the aforementioned use of schema.org types in the canvas.
Socio-Technical Frameworks and Evaluation Models
Beyond technical metadata, agentic automation projects should document the socio-technical context – how user expectations are elicited and met, how ethical considerations are integrated, and how success is measured over time. Several frameworks and models from HCI (Human–Computer Interaction), STS (Science & Technology Studies), and responsible innovation can inform metadata categories for these aspects:
•	Value Sensitive Design (VSD): VSD is an HCI framework that “systematically accounts for human values throughout the design process”[31]. It involves identifying stakeholders and their values, then iteratively designing and evaluating the technology to uphold those values. In a metadata context, one could include a “Values and Stakeholders” section in the canvas: listing key stakeholder groups (e.g. patients, doctors, developers, regulators) and the expectations or values they hold (e.g. privacy, accuracy, transparency). VSD suggests doing conceptual, empirical, and technical investigations – the outcomes of those can be captured as well (e.g., results of a user survey indicating which values are most important). By recording this, we align “User Expectations” in the canvas not just as raw requests, but grounded in stakeholder values. This data can later be used to compare how different projects balanced values like efficiency vs. autonomy, or to check if any value was neglected. While there isn’t a singular ontology for VSD, one could reuse terms from existing vocabularies (e.g., ethics ontologies or even schema.org’s ethicsPolicy property[32] to link to an ethics statement). The main point is to have structured entries for user needs/values and design decisions made to accommodate them.
•	Participatory Design & User Research: Many HCI methods (user personas, scenarios, user journey maps) produce artifacts that could be linked as metadata. For example, if the project created user personas to capture expectations, the canvas could reference these (perhaps as foaf:Person instances with extra attributes for persona characteristics). User stories (in agile format: “As a , I want ___ so that ”) represent expectation in a structured way and can be listed under requirements. There are even attempts to formalize user stories in RDF/ontology form in requirements engineering research[33]. While we need not adopt a specific one, we should allow for capturing such narratives or requirements in a comparable way. Each user story might have a status (implemented or not) – tying that to developer feasibility estimates (e.g., via a property like prov:wasDerivedFrom linking a user story to a development task in PROV). The canvas can thus track expectation fulfillment: each expectation either matched to implemented features or noted as out-of-scope due to feasibility issues (with rationale). This bridges the gap between user expectation and developer capability clearly.
•	Socio-Technical Integration Research (STIR): STIR comes from STS as a method to “embed social and ethical reflection within laboratory research… through structured intervention protocols”[34]. Essentially, it involves periodic reflective dialogues during a project to examine assumptions and impacts. For our purposes, we can include a metadata element for reflection logs or ethical assessments at each stage. For example, after each development sprint or project phase, a brief entry of what ethical or societal concerns were discussed and what decisions were made. This could even link to a PROV Activity representing an “Ethics review meeting” and an outcome Entity like an “Ethical risk assessment document”. By structuring these, the canvas not only records governance staging (with ethical gates) but also allows comparing how different projects conducted their reflections (frequency, depth, outcomes). STIR-inspired metadata ensures that “soft” governance aspects are not lost – they become queryable data (e.g., did the project involve stakeholders in mid-course corrections? can be answered if such events are logged).
•	Responsible Research and Innovation (RRI): RRI is a European framework emphasizing anticipation of impacts, inclusiveness, open science, and ethics throughout innovation[35][31]. Practically, RRI encourages documenting how a project addresses societal needs and expectations. In the canvas, we could have sections or tags for RRI keys: public engagement, gender/diversity considerations, sustainability, etc. For example, if the agentic automation is a clinical AI, did patients have input (public engagement)? Is the data diverse (avoiding bias)? These can be captured with yes/no or qualitative fields, possibly referencing standards (like the EU’s Ethics Guidelines for Trustworthy AI which list similar principles). Notably, Horizon Europe now embeds many RRI aspects as requirements[36] – ensuring compatibility with such requirements means the canvas could double as a reporting tool for funding compliance. An “Impact & Societal Considerations” category, with properties like impactAssessmentReport (link to any impact study) or simple flags for whether open science practices were followed, will help in later comparative analysis of projects’ socio-technical robustness. We should flag widely used models here, such as the AREA framework (Anticipate, Reflect, Engage, Act) from responsible innovation, or the Ethics Canvas (a tool similar to our canvas that prompts teams to think about impacts, stakeholders, risks). Including these frameworks’ elements in our schema (even if just as checklists or references) avoids missing any critical dimension of expectation vs. outcome.
•	Evaluation and Outcome Tracking: To track outcomes over time, we need to record not just the final result, but intermediate and post-deployment evaluations. HCI offers usability and UX evaluation metrics (e.g., System Usability Scale score, task completion rates) – these can be stored as properties under an “Evaluation” section. For instance, after user testing, a property usabilityScore: 85/100 or issueCount: 5 (usability issues found) could be logged. If the project uses an AI evaluation framework (say NIST’s AI Risk Management Framework or an internal KPI set), those metrics (accuracy, fairness measures, etc.) should be included in the outcomes metadata. We might borrow from the model card schema: Model Cards (Mitchell et al.) propose fields like intended use, performance metrics on various subsets, ethical considerations. While originally for ML models, the same idea can be applied to the entire agentic system. We could thus maintain a “Project Card” summarizing key evaluation points: did it meet the success criteria defined by users? What are the known limitations? How will it be monitored post-deployment? By structuring these (perhaps using JSON-LD similar to model cards or leveraging schema.org’s CreativeWork with specific description tags for each category), we support longitudinal tracking. One project might have multiple evaluation entries (preliminary lab test, pilot deployment, field trial, etc.), each with date and results – easily handled by PROV (each evaluation is an Activity that generates an EvaluationReport entity). This temporal chain of outcomes lets us see improvement or regression over time. For compatibility, we should ensure any metrics use standard definitions (e.g., if reporting accuracy, clarify if it’s overall accuracy, and perhaps link to the SKOS definition of that metric or the dataset it’s measured on).
•	Frameworks for Alignment: We should mention Technology Readiness Levels (TRL) as a common metric in research projects to gauge feasibility/maturity. The canvas could include a TRL at start and end (e.g., starting at TRL 3 and aiming for TRL 6). TRL is widely understood (Horizon Europe proposals often require it), so it helps contextualize developer feasibility estimates. Likewise, Human Readiness Level or Adoption readiness metrics have been proposed; if relevant, capturing those can illustrate alignment of user readiness with tech readiness. In EU biomedical projects, there’s often an implementation/evaluation framework like clinical validation stages – we should capture which stage the project reached (e.g., prototype, clinical trial, deployed service). A controlled vocabulary for stages (perhaps using the ISO 15224 or FDA phases for medtech) can be used.
In summary, socio-technical and evaluation metadata ensure that the Agentic Automation Canvas isn’t just a static design document, but a living record of expectations, decisions, and outcomes. By drawing on HCI/STS frameworks (VSD, participatory methods, STIR) we populate the canvas with fields that later enable qualitative comparison: e.g., comparing how much stakeholder engagement two projects did, or how they measured success (user satisfaction vs. purely technical metrics). These frameworks don’t always come with formal ontologies, but we can still standardize the categories (e.g., every project should list stakeholders, every project should report ethical issues encountered, etc.), which greatly improves reusability of the metadata.
Mapping Models to the Agentic Automation Canvas
To integrate the above standards without redundancy, we map them to the Canvas sections as follows (recommending reuse of existing schemas wherever possible):
•	Project Definition & Context: Use schema.org/Project (and ResearchProject) for core project info (title, description, objective)[1]. Extend with FRAPO for administrative details: e.g., frapo:Project with linked frapo:fundingGrant, frapo:leadOrganization, etc., to capture funding and partners[14][16]. Ensure each project has a unique ID (URI or DOI); adopting a FAIR Digital Object approach, the project itself can be a FAIR Digital Object with a PID and metadata (much like how Zenodo can mint DOIs for projects or outputs). This allows global referencing and linking to related objects (data, publications). Compatibility: Aligns with CERIF (via FRAPO) and schema.org – widely understood. Also include high-level classification (domain, keywords) possibly from existing taxonomies (e.g., MeSH for biomedical topics or ACM CCS for computing) to aid discovery.
•	User Expectations & Requirements: Represent these as prov:Plan or P-Plan elements linked to the project’s Activities. Each major user requirement or use-case can be a node (perhaps as a subclass of prov:Entity, since PROV treats Plan as Entity) describing the goal. Properties from requirements engineering ontologies or simple Dublin Core can capture the textual narrative of the requirement. If using agile user stories, include them as text plus metadata (priority, status). The Value Sensitive Design angle can be captured by tagging each requirement with the stakeholder/value it addresses (a simple link or annotation, e.g., Requirement X addresses value “privacy”). This could reuse the CSV (Core Values Ontology) if one exists, or just a controlled list of values. Mapping: User expectations map to prov:Plan (the plan guiding the automation)[13]; by using P-Plan’s p-plan:isStepOfPlan and related properties, we can detail sub-requirements and their relationships. This avoids redundancy by leveraging an existing plan schema instead of custom “Expectation” entries.
•	Developer Feasibility & Technical Assessment: Here we capture estimates like effort, technical risk, chosen algorithms, etc. Few standard ontologies explicitly cover “feasibility”, but we can repurpose project management schemas. For example, the PMO (Project Management Ontology) or similar (if available) could model tasks with attributes like estimatedDuration, requiredSkill, riskLevel. If not, simple Dublin Core or schema.org properties might suffice (e.g., schema.org ArchiveComponent or SoftwareApplication for components with an algorithm property). We should align any feasibility rating with known scales: e.g., TRL as mentioned (we could use an attribute trlLevel from a vocabulary, or dct:subject “TRL 5”). Risk can be linked to standards like ISO 31000 risk categories or simply recorded as Low/Med/High. Mapping: This section doesn’t have a direct W3C ontology, but PROV can still be helpful – one could model a planning Activity where the developer produces a feasibility report (Entity). Using PROV-O for this means we link the report to the Plan (prov:wasDerivedFrom Plan). We also avoid redundancy by linking to known tech descriptors: for instance, if the project uses a machine learning model, reference ONNX or PMML for the model, or use skos:Concept identifiers for algorithms (like “BERT” or “PCR test”). The key is to not free-form describe tech when an ontology exists (e.g., use EDAM ontology terms for algorithms in bioinformatics, etc.). This ensures our feasibility discussions are interoperable (someone could query all projects that rated a certain algorithm as high-risk).
•	Governance & Staging: Map each stage to a PROV Activity with a type/stage label. We can use a simple taxonomy for stages: e.g., {Design, Development, Validation, Deployment, Monitoring}. Each stage-activity can have associated Agents (who leads it) and maybe a prov:Plan (milestone goals for that stage). Notably, for compliance, if there are formal ethics reviews or regulatory approvals, represent those as Activities too (e.g., an “IRB Approval” activity with an outcome entity “IRB certificate”). Use PROV’s timing properties to sequence these[37]. By doing so, the canvas essentially forms a timeline of provenance for the project itself, which can be visualized or analyzed. Many organizations have governance frameworks (like AI project maturity models[38]); if one is widely used (e.g., IBM’s AI governance stages), the stage names can align to that to ease understanding. Compatibility: Using PROV for stages means we’re consistent with provenance standards (and could merge the project timeline with the provenance of data processing, for end-to-end traceability). Also, by referencing governance documents through standard identifiers (say an ISO standard for risk management), we avoid local jargon. The canvas should flag any regulatory standards applicable (for biomedical, e.g., GxP, HIPAA, GDPR) – possibly via the COBRA ontology or similar if available, or just a skos:Concept from regulatory code lists.
•	Data Access & Sensitivity: Use DCAT/DCAT-AP to describe each dataset (as mentioned). Attach DUO terms for usage restrictions[24] and use dct:accessRights with values like “open” or “restricted”. If needed, include a reference to the actual Data Use Agreement or consent (as a CreativeWork). Ensuring the data metadata is as per FAIR principles, we link persistent IDs (DOIs or accession numbers). If the project is in EOSC context, adopting the EOSC PID policy (i.e., all significant entities have a PID) will improve reusability – the canvas could list the PIDs of datasets, models, etc., rather than just names. Mapping: Canvas data entries become rich DCAT records, which can be serialized to JSON-LD for integration with catalogs. No redundancy since DCAT is purpose-built; we only add project-specific context like which task uses which dataset (that linking is done via PROV: an Activity uses a Dataset entity).
•	Outcomes & Evaluation: For immediate outcomes (deliverables), we leverage FRAPO’s deliverable class for promised outputs[15], and FaBiO or schema.org CreativeWork for publications, reports, etc. Each outcome can have a relation to the task that produced it (prov:wasGeneratedBy). For evaluation results, we treat them as specialized outcomes: e.g., a “UsabilityTestResult” document, or simply data points. We can use schema.org’s Measure or Observation (if available) or the W3C DataCube vocabulary for structured results. In simpler terms, a table of metrics can be a CSV linked as a Dataset, or we annotate key metrics in the metadata (e.g., a property accuracy = 0.95 on the outcome entity, with context of dataset used). The RO-Crate approach is very useful here: RO-Crate allows bundling data, code, and results with JSON-LD descriptions, and indeed “an RO-Crate is an integrated view of the methods, data, output and outcomes of a project, linking all together to share research outputs with their context”[39]. We should strive to package the agentic project’s core outputs in an RO-Crate compliant way. That means our canvas metadata can essentially be the RO-Crate metadata file (which uses schema.org/PROV), ensuring that anyone can unzip the crate and immediately have a full picture of what was done, why, and what resulted. Adopting RO-Crate standards (for example, marking the root as a Dataset, linking to all files, and including provenance in the JSON) will avoid duplicating effort in creating separate project reports and data packages – it’s one unified representation. It also aligns with the FAIR Digital Object vision by treating the entire project outputs as a digital object with rich metadata[40][41].
•	Widely-Used Models Compatibility: We explicitly flag a few important models to ensure our canvas can interoperate:
•	FAIR Digital Objects (FDO): Each key entity in the canvas (project, dataset, software agent, outcome) should be identifiable (with PIDs) and have rich metadata attached, as per FDO principles. This implies using standards like PROV, DCAT, schema.org – which we are – because “the FDO concept is agnostic about content but requires rich metadata for findability”[42][40]. By adhering to these and perhaps including an RDA “kernel metadata” (a minimal set of attributes for any object), we make our project canvas entries ready to plug into a FAIR ecosystem (e.g., an EOSC portal or a Crossref/Datacite catalog).
•	EOSC & RDA Outputs: We align with DCAT-AP for data and the RDA maDMP for data plans, as discussed, which are recommended by EOSC. If there are specific EOSC profiles for software or workflows (e.g., the EOSC Software Catalog might use CodeMeta or SPDX for software description), we should use those for any agent software component. Similarly, the Research Data Alliance’s metadata catalog (RDAMS) lists many domain standards – we have picked those relevant (OBI, ISA for bio, etc.). Continual crosswalk with RDA’s Metadata Standards Directory[43] can ensure we didn’t miss a domain-specific schema that could be reused.
•	Croissant: Already covered – using it for ML datasets will directly ensure compatibility with ML commons repositories and even allow using Croissant tools to validate our dataset descriptions[44].
•	CodeMeta: If the agentic automation involves software (very likely), CodeMeta is a JSON-LD schema (aligned with schema.org) for software metadata (programming language, dependencies, runtime requirements, etc.). It’s widely used for research software including by platforms like GitHub and Zenodo. We should consider representing any software artifact of the project (the agent itself, or scripts) using CodeMeta terms. This prevents redundancy in documenting software properties and makes it easier to publish the software to repositories.
•	Workflow standards: If the automation is orchestrated as a workflow, consider Common Workflow Language (CWL) or WorkflowRO metadata to describe its steps. PROV and P-Plan already give us a backbone for this, but something like ProvOne (an extension of PROV for scientific workflows) could add detail (e.g., distinguishing subworkflow vs atomic step, etc.). Many workflow systems output PROV or RO-Crate, so aligning with those will improve reusability of our metadata in workflow repositories.
In implementing the canvas, we should avoid duplication by linking across these models. For example, don’t separately list a dataset in a “Data” section and again in a “Provenance” section – instead, have one node (DCAT Dataset with schema.org/Croissant attributes) and reference it from the plan and task records. Use unique identifiers to refer to the same entity across schemas (e.g., a dataset has one URI used in DCAT, PROV, schema.org contexts). Similarly, a deliverable could be both a FRAPO deliverable and a schema.org CreativeWork; we can assert both types on the same identifier rather than create two. By mapping each canvas field to an existing ontology property, we ensure that when comparing projects, we’re comparing on standard grounds (e.g., all projects have prov:wasAssociatedWith relations linking to Persons for contributors, so we can count stakeholder types easily).
Finally, the canvas should remain extensible – if a particular project finds a schema that better captures its nuance (say an HCI project might use the CHIMetadata schema for study data), it can plug in those terms. Our mapping above covers the core concepts common to agentic automation in research/biomed, but the design is modular. The use of schema.org and W3C standards as a backbone ensures that new extensions (e.g., a new ontology for AI accountability) can integrate without breaking the structure. The result will be a unified, reusable metadata schema for agentic systems, grounded in well-adopted standards and enriched by socio-technical descriptors, enabling rich comparative analysis of how these systems are scoped, built, and evaluated across different projects.
Sources: The integration approach builds on established vocabularies like Schema.org[1], W3C DCAT[5], PROV-O[45] and its Plan extension[13], as well as research project ontologies like FRAPO[14]. We incorporate data governance standards such as RDA’s machine-actionable DMP schema[26] and GA4GH’s DUO for consent permissions[24]. Emerging community standards like Croissant for ML datasets[27] and RO-Crate for packaging research objects[39] ensure the canvas remains interoperable with modern data science practices. We also draw on socio-technical frameworks – e.g. STIR for reflective governance[34] and VSD for values-driven design[31] – to inform metadata fields that capture user–developer alignment over time. This blended approach leverages the strengths of each model, mapping them onto the Agentic Automation Canvas so that redundancy is minimized and reusability is maximized across projects and platforms. [14][13][27][39]
 
[1] [3] [32] Project - Schema.org Type
https://schema.org/Project
[2] ResearchProject - Schema.org Type
https://schema.org/ResearchProject
[4] Bioschemas - Bioschemas
http://bioschemas.org/
[5] Data Catalog Vocabulary (DCAT) - Version 2
https://www.w3.org/TR/vocab-dcat-2/
[6] Our metadata model - Documentation of the European Data ... - GitLab
https://dataeuropa.gitlab.io/data-provider-manual/our-metadata-model/
[7] [8] [9] [10] [11] [37] [45] PROV-O: The PROV Ontology
https://www.w3.org/TR/prov-o/
[12] [13] The P-Plan Ontology
https://www.opmw.org/model/p-plan/
[14] [15] [16] [17] [18] [23] FRAPO, the Funding, Research Administration and Projects Ontology
https://sparontologies.github.io/frapo/current/frapo.html
[19] The Ontology for Biomedical Investigations | PLOS One
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0154556
[20] OBI (Ontology for Biomedical Investigations) - OBO Foundry
http://obofoundry.org/ontology/obi.html
[21] [22] Microsoft Word - Ontology_publication_v2.0.docx
http://publications.europa.eu/resource/cellar/369859bb-3611-11eb-b27b-01aa75ed71a1.0001.01/DOC_1
[24] Data Use Ontology (DUO) – GA4GH
https://www.ga4gh.org/product/data-use-ontology-duo/
[25] Aligning NIH's existing data use restrictions to the GA4GH DUO ...
https://pmc.ncbi.nlm.nih.gov/articles/PMC10504671/
[26] rd-alliance.org
https://www.rd-alliance.org/sites/default/files/Card%20RDA_DMPCommon_Standard_for_Machine-actionable_Data_Management_Plans_June2020.pdf
[27] [28] [29] [30] [44] Croissant - MLCommons
https://mlcommons.org/working-groups/data/croissant/
[31] [34] [35] [36] Responsible Research and Innovation - Wikipedia
https://en.wikipedia.org/wiki/Responsible_Research_and_Innovation
[33] [PDF] An Ontology Framework for Generating Requirements Specification
https://ijaseit.insightsociety.org/index.php/ijaseit/article/download/10164/pdf_1448
[38] AI Governance Maturity Model: The 5 Stages of Evolution
https://www.projectmanagertemplate.com/post/ai-governance-maturity-model-the-5-stages-of-evolution
[39] GitHub - ResearchObject/ro-crate-r: RO-Crate library for R
https://github.com/ResearchObject/ro-crate-r
[40] [41] [42] rd-alliance.org
https://www.rd-alliance.org/sites/default/files/damdid2018_paper_schultes_wbg-final.pdf
[43] [PDF] Domain Specific Metadata Profiles - EOSC CZ
https://www.eosc.cz/media/4023945/cernohlavkova_sel_dom_spec_metadata-model_ostrava_conference_3_12_2025_final.pdf
